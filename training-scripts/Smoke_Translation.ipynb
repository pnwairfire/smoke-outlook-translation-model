{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9146f6-2504-4284-b458-dae992c94906",
   "metadata": {},
   "source": [
    "### Package Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c478d-fed9-4481-a5c8-551e77e82721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import jsons\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import sacrebleu\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef242f2e-3a87-448b-b5d1-88e14b8b4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c17206-0e1f-43fa-a618-0b5782183edd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "# Technical Translation: English to Spanish, GPT 4\n",
    "---\n",
    "\n",
    "## Description\n",
    "The USFS AirFire group maintains systems that forecasters use to create Smoke Outlooks, i.e. wildfire smoke forecasts. These are initially written in English. Then they are auto-translated into Spanish with ChatGPT. Finally, the translations are manually edited by a native speaker. The goal of this project is to use our corpus of existing outlooks and translations to train a ChatGPT model for improved translations. A simple user interface will also be needed to allow professional translators to compare translations using the trained and untrained models.\n",
    "\n",
    "## Goals\n",
    "We hope to reduce the amount of time staff spend translating outlooks while maintaining high quality translations. If all goes well, we will end up with: 1) a trained model for improved translations; 2) tailored prompts; and 3) a small user interface to test ChatGPT models. If successful, this approach will be used to support translation targets other than Spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c9736-3b4b-48f1-85d3-31eea73da86a",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "---\n",
    "## Data Preparation\n",
    "The following data contains historical smoke and fire forecasts that have been translated from English to Spanish. These translations have been verified by a translator.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d3e3a-614a-4f9b-a4aa-d46973c9252e",
   "metadata": {},
   "source": [
    "**Comment:** you may need to change the paths for where the data is being pulled from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186dfd8-a458-48fc-82f3-94cc3fa59a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = pd.read_csv('./data/Location_Forecast_translations.csv')\n",
    "collection_df = pd.read_csv('./data/Collection_Forecast_translations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456c783-ca4f-4997-8ce9-5a168b70a36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5bfc5-f3ac-44dd-aa01-aad06de962a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collection_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328cbaf8-8ad5-4b5a-939e-f3314ab08f10",
   "metadata": {},
   "source": [
    "**Comment:** Edits to the translation prompt can be made here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feccdf64-e6b0-4acb-ad5c-1d6f52fa0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = \"\"\"\n",
    "You are an official English to Spanish translator translating air quality forecasts\n",
    "from English into Spanish. Use the following word-translation pairs when translating:\n",
    "\n",
    "GOOD => BUENO\n",
    "MODERATE => MODERADA\n",
    "USG => IGS\n",
    "UNHEALTHY => INSALUBRE\n",
    "VERY UNHEALTHY => MUY INSALUBRE\n",
    "HAZARDOUS => PELIGROSA\n",
    "NaN => NaN\n",
    "\n",
    "Translate the following air quality discussion into Spanish:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c43a0-34c6-4bbc-83af-22bfd53bdbfc",
   "metadata": {},
   "source": [
    "**Comment:** The following functions are used to load translations from .csv format to a usable .jsonl format. This is required for OpenAI API usage.\n",
    "\n",
    "The function write_to_jsonl() saves the jsonl files in the current working directory. The names of the training, validation, and testing files can be changed when the function is actually called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c825c2b-da47-4c43-8a97-e7d7b634318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_location_forecast(location_data, translation_prompt, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepares training, testing, and validation datasets for location forecast translations.\n",
    "    \n",
    "    The function splits the input dataset into training (70%), validation (15%), and testing (15%) sets.\n",
    "    It formats data into structured messages for a translation model.\n",
    "    \n",
    "    Parameters:\n",
    "    location_data (DataFrame): A pandas DataFrame containing English and Spanish forecast texts.\n",
    "    translation_prompt (str): The system prompt for guiding the translation model.\n",
    "    random_state (int, optional): Random seed for reproducibility. Default is 42.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing three lists:\n",
    "        - location_training_data (list): Training data formatted as system, user, and assistant messages.\n",
    "        - location_testing_data (list): Testing data containing paired English and Spanish texts.\n",
    "        - location_validation_data (list): Validation data formatted similarly to training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data, temp_data = train_test_split(location_data, test_size=0.3, random_state=random_state)\n",
    "    validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=random_state)\n",
    "\n",
    "    # prepare training data\n",
    "    location_training_data = []\n",
    "    for _, row in train_data.iterrows():\n",
    "        english_text = f\"\"\"\n",
    "Forecast Summary: {row['forecast_summary']}\n",
    "Today's Comment: {row['today_comment']}\n",
    "Tomorrow's Comment: {row['tomorrow_comment']}\n",
    "Extended Comment: {row['extended_comment']}\"\"\"\n",
    "    \n",
    "        spanish_text = f\"\"\"\n",
    "Resumen del Pronóstico: {row['forecast_summary_es']}\n",
    "Comentario de Hoy: {row['today_comment_es']}\n",
    "Comentario de Mañana: {row['tomorrow_comment_es']}\n",
    "Comentario Extendido: {row['extended_comment_es']}\"\"\"\n",
    "    \n",
    "        location_training_data.append({\n",
    "            'messages': [\n",
    "                {'role': 'system', 'content': translation_prompt}, \n",
    "                {'role': 'user', 'content': english_text}, \n",
    "                {'role': 'assistant', 'content': spanish_text}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # prepare testing data\n",
    "    location_testing_data = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        english_text = f\"\"\"\n",
    "Forecast Summary: {row['forecast_summary']}\n",
    "Today's Comment: {row['today_comment']}\n",
    "Tomorrow's Comment: {row['tomorrow_comment']}\n",
    "Extended Comment: {row['extended_comment']}\"\"\"\n",
    "    \n",
    "        spanish_text = f\"\"\"\n",
    "Resumen del Pronóstico: {row['forecast_summary_es']}\n",
    "Comentario de Hoy: {row['today_comment_es']}\n",
    "Comentario de Mañana: {row['tomorrow_comment_es']}\n",
    "Comentario Extendido: {row['extended_comment_es']}\"\"\"\n",
    "    \n",
    "        location_testing_data.append({\n",
    "            'english_text': english_text,\n",
    "            'spanish_text': spanish_text\n",
    "        })\n",
    "\n",
    "    # prepare validation data\n",
    "    location_validation_data = []\n",
    "    for _, row in validation_data.iterrows():\n",
    "        english_text = f\"\"\"\n",
    "Forecast Summary: {row['forecast_summary']}\n",
    "Today's Comment: {row['today_comment']}\n",
    "Tomorrow's Comment: {row['tomorrow_comment']}\n",
    "Extended Comment: {row['extended_comment']}\"\"\"\n",
    "    \n",
    "        spanish_text = f\"\"\"\n",
    "Resumen del Pronóstico: {row['forecast_summary_es']}\n",
    "Comentario de Hoy: {row['today_comment_es']}\n",
    "Comentario de Mañana: {row['tomorrow_comment_es']}\n",
    "Comentario Extendido: {row['extended_comment_es']}\"\"\"\n",
    "    \n",
    "        location_validation_data.append({\n",
    "            'messages': [\n",
    "                {'role': 'system', 'content': translation_prompt}, \n",
    "                {'role': 'user', 'content': english_text}, \n",
    "                {'role': 'assistant', 'content': spanish_text}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    return location_training_data, location_testing_data, location_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a553553-3a72-4931-a3e4-f95b2b0560e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_collection_forecast(collection_data, translation_prompt, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepares training, testing, and validation datasets for collection forecast translations.\n",
    "    \n",
    "    The function splits the input dataset into training (70%), validation (15%), and testing (15%) sets.\n",
    "    It formats data into structured messages for a translation model.\n",
    "    \n",
    "    Parameters:\n",
    "    collection_data (DataFrame): A pandas DataFrame containing English and Spanish forecast texts.\n",
    "    translation_prompt (str): The system prompt for guiding the translation model.\n",
    "    random_state (int, optional): Random seed for reproducibility. Default is 42.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing three lists:\n",
    "        - collection_training_data (list): Training data formatted as system, user, and assistant messages.\n",
    "        - collection_testing_data (list): Testing data containing paired English and Spanish texts.\n",
    "        - collection_validation_data (list): Validation data formatted similarly to training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data, temp_data = train_test_split(collection_data, test_size=0.3, random_state=random_state)\n",
    "    validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=random_state)\n",
    "\n",
    "    # prepare training data\n",
    "    collection_training_data = []\n",
    "    for _, row in train_data.iterrows():\n",
    "        english_text = f\"\"\"\n",
    "Fire Statement: {row['fire_statement_json']}\n",
    "Smoke Statement: {row['smoke_statement_json']}\n",
    "Special Statement: {row['special_statement_json']}\"\"\"\n",
    "\n",
    "        spanish_text = f\"\"\"\n",
    "Declaración del Incendio: {row['fire_statement_json_es']}\n",
    "Declaración del Humo: {row['smoke_statement_json_es']}\n",
    "Declaración Especial: {row['special_statement_json_es']}\"\"\"\n",
    "\n",
    "        collection_training_data.append({\n",
    "            'messages': [\n",
    "                {'role': 'system', 'content': translation_prompt}, \n",
    "                {'role': 'user', 'content': english_text}, \n",
    "                {'role': 'assistant', 'content': spanish_text}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # prepare testing data\n",
    "    collection_testing_data = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        english_text = f\"\"\"\n",
    "Fire Statement: {row['fire_statement_json']}\n",
    "Smoke Statement: {row['smoke_statement_json']}\n",
    "Special Statement: {row['special_statement_json']}\"\"\"\n",
    "\n",
    "        spanish_text = f\"\"\"\n",
    "Declaración del Incendio: {row['fire_statement_json_es']}\n",
    "Declaración del Humo: {row['smoke_statement_json_es']}\n",
    "Declaración Especial: {row['special_statement_json_es']}\"\"\"\n",
    "\n",
    "        collection_testing_data.append({\n",
    "            'english_text': english_text,\n",
    "            'spanish_text': spanish_text\n",
    "        })\n",
    "\n",
    "    # prepare validation data\n",
    "    collection_validation_data = []\n",
    "    for _, row in validation_data.iterrows():\n",
    "        english_text = f\"\"\"\n",
    "Fire Statement: {row['fire_statement_json']}\n",
    "Smoke Statement: {row['smoke_statement_json']}\n",
    "Special Statement: {row['special_statement_json']}\"\"\"\n",
    "\n",
    "        spanish_text = f\"\"\"\n",
    "Declaración del Incendio: {row['fire_statement_json_es']}\n",
    "Declaración del Humo: {row['smoke_statement_json_es']}\n",
    "Declaración Especial: {row['special_statement_json_es']}\"\"\"\n",
    "\n",
    "        collection_validation_data.append({\n",
    "            'messages': [\n",
    "                {'role': 'system', 'content': translation_prompt}, \n",
    "                {'role': 'user', 'content': english_text}, \n",
    "                {'role': 'assistant', 'content': spanish_text}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    return collection_training_data, collection_testing_data, collection_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb249f4d-d34c-40f2-b92d-6a424dd0cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_jsonl(datasets, file_name):\n",
    "    \"\"\"\n",
    "    Writes multiple datasets to a JSONL (JSON Lines) file.\n",
    "    \n",
    "    Each entry from the provided datasets is written as a separate line in the JSONL file.\n",
    "    \n",
    "    Parameters:\n",
    "    datasets (list of lists): A list containing multiple datasets, where each dataset is a list of dictionaries.\n",
    "    file_name (str): The base name of the output file (without extension).\n",
    "    \n",
    "    Returns:\n",
    "    str: The file path of the saved JSONL file.\n",
    "    \"\"\"\n",
    "\n",
    "    # directory to which files are saved to can be changed here.\n",
    "    # data saved in same directory as notebook\n",
    "    output_path = f'{file_name}.jsonl'\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        for dataset in datasets:\n",
    "            for entry in dataset:\n",
    "                json.dump(entry, file)\n",
    "                file.write('\\n')\n",
    "    print('File saved at:', output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b1b9a-05fb-45ba-ba74-4bd23a07ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "loc_train, loc_test, loc_val = prepare_location_forecast(location_df, translation_prompt)\n",
    "coll_train, coll_test, coll_val = prepare_collection_forecast(collection_df, translation_prompt)\n",
    "\n",
    "# group data\n",
    "training_datasets = [loc_train, coll_train]\n",
    "testing_datasets = [loc_test, coll_test]\n",
    "validation_datasets = [loc_val, coll_val]\n",
    "\n",
    "# merge data to jsonl\n",
    "combined_training_dataset_path = write_to_jsonl(training_datasets, 'combined_training_dataset')\n",
    "combined_testing_dataset_path = write_to_jsonl(testing_datasets, 'combined_testing_dataset')\n",
    "combined_validation_dataset_path = write_to_jsonl(validation_datasets, 'combined_validation_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ead359-00c8-4a06-a600-397e54595e0a",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "---\n",
    "## Data Validation\n",
    "Now that the training data has been validated, validation and cost estimation procedures ([found here](https://cookbook.openai.com/examples/chat_finetuning_data_prep)) will be used to check for format errors, provide basic statistics, and estimate token counts for fine-tuning costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee05f5-8a48-4257-8f6e-26268ce72335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "combined_training_dataset = load_dataset(combined_training_dataset_path)\n",
    "combined_testing_dataset = load_dataset(combined_testing_dataset_path)\n",
    "combined_validation_dataset = load_dataset(combined_validation_dataset_path)\n",
    "\n",
    "print('Number of training examples:', len(combined_training_dataset))\n",
    "print('Number of testing examples:', len(combined_testing_dataset))\n",
    "print('Number of validation examples:', len(combined_validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f1ca2-b982-484d-8757-c36add14662d",
   "metadata": {},
   "source": [
    "**Comment:** Testing Data will give an error for missing message list. This is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf78b95-9a1c-49dd-a3a1-03f984a27a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_format(dataset):\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "    \n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "            \n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "            \n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            \n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            \n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "                \n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "            \n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        \n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "    \n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")\n",
    "        \n",
    "print('Training Data')\n",
    "check_format(combined_training_dataset)\n",
    "# missing messages in testing is fine\n",
    "print('\\nTesting Data')\n",
    "check_format(combined_testing_dataset)\n",
    "print('\\nValidation Data')\n",
    "check_format(combined_validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de730a4-dac0-4ab1-9a35-8324e5c23707",
   "metadata": {},
   "source": [
    "## Token Counting and Cost Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1b6b9-606b-4645-b335-d50edc736cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060d107-16ad-4d06-9938-e5fa05190748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warnings_counts_cost(dataset):\n",
    "    # Warnings and tokens counts\n",
    "    n_missing_system = 0\n",
    "    n_missing_user = 0\n",
    "    n_messages = []\n",
    "    convo_lens = []\n",
    "    assistant_message_lens = []\n",
    "    \n",
    "    for ex in dataset:\n",
    "        messages = ex[\"messages\"]\n",
    "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "            n_missing_system += 1\n",
    "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "            n_missing_user += 1\n",
    "        n_messages.append(len(messages))\n",
    "        convo_lens.append(num_tokens_from_messages(messages))\n",
    "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "        \n",
    "    print(\"Num examples missing system message:\", n_missing_system)\n",
    "    print(\"Num examples missing user message:\", n_missing_user)\n",
    "    print_distribution(n_messages, \"num_messages_per_example\")\n",
    "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "    n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "    print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "    # Pricing and default n_epochs estimate\n",
    "    MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "    \n",
    "    TARGET_EPOCHS = 3\n",
    "    MIN_TARGET_EXAMPLES = 100\n",
    "    MAX_TARGET_EXAMPLES = 25000\n",
    "    MIN_DEFAULT_EPOCHS = 1\n",
    "    MAX_DEFAULT_EPOCHS = 25\n",
    "    \n",
    "    n_epochs = TARGET_EPOCHS\n",
    "    n_train_examples = len(dataset)\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "    \n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "    print(f\"\\nDataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "    print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "    print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "\n",
    "warnings_counts_cost(combined_training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f378f-7a74-42eb-a253-04ef69b7242c",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "---\n",
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c93e0e-a7c9-4ef9-9cc5-905f7417f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b71ab-5683-4eea-a9e6-06d737371883",
   "metadata": {},
   "source": [
    "**Comment:** training and validation data both need to be uploaded separately for fine-tuning. The function file_upload() takes the file paths returned earlier as inputs and returns the corresponding file IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e134c5-b0b3-4be3-ac2b-7b1f32a31983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_upload(dataset_path):\n",
    "    \"\"\"\n",
    "    Uploads a dataset file for fine-tuning using the client API.\n",
    "\n",
    "    Parameters:\n",
    "    dataset_path (str): The file path of the dataset to be uploaded.\n",
    "\n",
    "    Returns:\n",
    "    str: The ID of the uploaded file, which can be used for fine-tuning.\n",
    "\n",
    "    Example:\n",
    "    >>> file_id = file_upload(\"training_dataset.jsonl\")\n",
    "    Fine-tuning file ID: file-abc123\n",
    "    \"\"\"\n",
    "    file_response = client.files.create(\n",
    "        file=open(dataset_path, \"rb\"),\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "    file_id = file_response.id\n",
    "    print(f'Fine-tuning file ID: {file_id}')\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aace32-caf9-4287-975c-38980cae9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_id = file_upload(combined_training_dataset_path)\n",
    "vf_id = file_upload(combined_validation_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ed536-0d69-480f-9e25-668c453c1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fine_tune_job(model_name, training_file_id, validation_file_id=None, seed=42,\n",
    "                        batch_size='auto', learning_rate_multiplier='auto', n_epochs='auto'):\n",
    "    \"\"\"\n",
    "    Creates a fine-tuning job for the specified model using the given training and optional validation files.\n",
    "\n",
    "    Parameters:\n",
    "    model_name (str): The name of the base model to fine-tune.\n",
    "    training_file_id (str): The ID of the uploaded training file.\n",
    "    validation_file_id (str, optional): The ID of the uploaded validation file (default: None).\n",
    "    seed (int, optional): Random seed for reproducibility (default: 42).\n",
    "    batch_size (str or int, optional): Batch size for training (default: 'auto').\n",
    "    learning_rate_multiplier (str or float, optional): Learning rate multiplier (default: 'auto').\n",
    "    n_epochs (str or int, optional): Number of training epochs (default: 'auto').\n",
    "\n",
    "    Returns:\n",
    "    str: The ID of the created fine-tuning job.\n",
    "\n",
    "    Example:\n",
    "    >>> job_id = create_fine_tune_job(\"gpt-3.5-turbo\", \"file-abc123\")\n",
    "    Fine-tuning job created with ID: job-xyz789\n",
    "    \"\"\"\n",
    "    fine_tune_job = client.fine_tuning.jobs.create(\n",
    "        seed=seed,\n",
    "        training_file=training_file_id,\n",
    "        validation_file=validation_file_id,\n",
    "        model=model_name,\n",
    "        method={\n",
    "            'type': 'supervised',\n",
    "            'supervised': {\n",
    "                'hyperparameters': {\n",
    "                    'batch_size': batch_size,\n",
    "                    'learning_rate_multiplier': learning_rate_multiplier,\n",
    "                    'n_epochs': n_epochs\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Fine-tuning job created with ID: {fine_tune_job.id}\")\n",
    "    return fine_tune_job.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056cc19-b818-43c9-bb3a-0a0eabf59e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftj_id = create_fine_tune_job(model_name='gpt-4o-mini-2024-07-18',\n",
    "                             training_file_id=tf_id,\n",
    "                             validation_file_id=vf_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b15f72-4122-47ba-a546-952637755582",
   "metadata": {},
   "source": [
    "**Comment:** If you would like to explore the training and testing results without fully retaining a model, save the fine tune job id and paste it below. This allows you to get specific models based on the fine tuning job id and conduct further testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a162bd-3d9c-4ab8-9704-772241f84e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftj_id = 'ftjob-WoNQoiX3pnfVcLaQsAyz5EAg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ecbbb-0c35-46ad-baca-f35ad23b06a8",
   "metadata": {},
   "source": [
    "**Comment:** Depending on the size of the dataset, finetuning may take a while! You can check on the progress of the fine tuning job using the line of code below. \n",
    "\n",
    "In the training job response, the .data dictionary contains all of the metrics on progress, accuracy, loss, etc. 'total_steps' shows how many training steps are required for the job. When calling the progress report, set the limit to >= 'total_steps'. Once 'step' = 'total_steps', training is complete.\n",
    "\n",
    "**To-do:** TQDM progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec0f3d-b39d-490e-b5a3-980a46cd97ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wait until training is complete\n",
    "# may take a while\n",
    "print(client.fine_tuning.jobs.list_events(fine_tuning_job_id=ftj_id, limit=2100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181b7be-d1d9-4c19-bae8-59a41d2d2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_metrics(fine_tune_job_id, limit=2100):\n",
    "    \"\"\"\n",
    "    Retrieves and processes fine-tuning job metrics, including training loss, validation loss, \n",
    "    and token accuracy, from the job's event logs.\n",
    "\n",
    "    Parameters:\n",
    "    fine_tune_job_id (str): The ID of the fine-tuning job.\n",
    "    limit (int, optional): The maximum number of event records to fetch (default: None).\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing six lists:\n",
    "        - model_train_loss (list[float]): Training loss values.\n",
    "        - model_valid_loss (list[tuple(int, float)]): Validation loss values with steps.\n",
    "        - model_full_valid_loss (list[tuple(int, float)]): Full validation loss values with steps.\n",
    "        - model_train_mean_token_accuracy (list[float]): Training mean token accuracy values.\n",
    "        - model_valid_mean_token_accuracy (list[tuple(int, float)]): Validation mean token accuracy values with steps.\n",
    "        - model_full_valid_mean_token_accuracy (list[tuple(int, float)]): Full validation mean token accuracy values with steps.\n",
    "\n",
    "    Example:\n",
    "    >>> mtl, mvl, mfvl, mtmta, mvmta, mfvmta = get_training_metrics(fine_tune_job_id=ftj_id, limit=2100)\n",
    "    \"\"\"\n",
    "    fine_tune_response = client.fine_tuning.jobs.list_events(fine_tuning_job_id=ftj_id, limit=limit)\n",
    "    event_data = fine_tune_response.data\n",
    "    event_data.reverse()\n",
    "    \n",
    "    model_train_loss = []\n",
    "    model_valid_loss = [(0,1)]\n",
    "    model_full_valid_loss = [(0,1)]\n",
    "    model_train_mean_token_accuracy = []\n",
    "    model_valid_mean_token_accuracy = []\n",
    "    model_full_valid_mean_token_accuracy = []\n",
    "    \n",
    "    # Iterate over each event and append values if they exist in event.data\n",
    "    for event in event_data:\n",
    "        if event.data:\n",
    "            if 'train_loss' in event.data:\n",
    "                model_train_loss.append(event.data['train_loss'])\n",
    "            if 'valid_loss' in event.data:\n",
    "                model_valid_loss.append((event.data['step'], event.data['valid_loss']))\n",
    "            if 'full_valid_loss' in event.data:\n",
    "                model_full_valid_loss.append((event.data['step'], event.data['full_valid_loss']))\n",
    "            if 'train_mean_token_accuracy' in event.data:\n",
    "                model_train_mean_token_accuracy.append(event.data['train_mean_token_accuracy'])\n",
    "            if 'valid_mean_token_accuracy' in event.data:\n",
    "                model_valid_mean_token_accuracy.append((event.data['step'], event.data['valid_mean_token_accuracy']))\n",
    "            if 'full_valid_mean_token_accuracy' in event.data:\n",
    "                model_full_valid_mean_token_accuracy.append((event.data['step'], event.data['full_valid_mean_token_accuracy']))\n",
    "\n",
    "    return model_train_loss, model_valid_loss, model_full_valid_loss, model_train_mean_token_accuracy, model_valid_mean_token_accuracy, model_full_valid_mean_token_accuracy\n",
    "\n",
    "mtl, mvl, mfvl, mtmta, mvmta, mfvmta = get_training_metrics(fine_tune_job_id=ftj_id, limit=2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0ca26-84b2-4603-864c-0b28e46065f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=4):\n",
    "    \"\"\"\n",
    "    Computes the moving average of a given dataset using a specified window size.\n",
    "\n",
    "    Parameters:\n",
    "    data (list[float]): The input list of numerical values.\n",
    "    window_size (int, optional): The number of consecutive values to consider for averaging (default: 4).\n",
    "\n",
    "    Returns:\n",
    "    list[float]: A list of averaged values, where each value is the mean of `window_size` consecutive elements.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    m_avg = []\n",
    "    while i < (len(data) - window_size + 1):\n",
    "        window = data[i:i + window_size]\n",
    "        window_average = round(sum(window) / window_size, 2)\n",
    "        m_avg.append(window_average)\n",
    "        i += 1\n",
    "    \n",
    "    return m_avg\n",
    "\n",
    "mtl_smooth = moving_average(mtl, window_size=12)\n",
    "mtmta_smooth = moving_average(mtmta, window_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c96aa-4d4c-4a6b-9c5c-bd7ba7bce472",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mtl, label='Training Loss', color='b')\n",
    "plt.plot(mtl_smooth, label='Moving Average (Window=12)', color='r')\n",
    "plt.title('Model Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x, y = zip(*mfvl)\n",
    "plt.plot(x, y, label='Full Validation Loss', color='cyan', marker='x')\n",
    "x, y = zip(*mvl)\n",
    "plt.plot(x, y, label='Validation Loss', color='orange', marker = '+')\n",
    "plt.title('Model Validation Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(mtmta, label='Mean Training Accuracy', color='b')\n",
    "plt.plot(mtmta_smooth, label='Moving Average (Window=12)', color='r')\n",
    "plt.title('Model Mean Token Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mvmta, mfvmta\n",
    "x, y = zip(*mfvmta)\n",
    "plt.plot(x, y, label='Full Mean Validation Accuracy', color='cyan', marker='x')\n",
    "x, y = zip(*mvmta)\n",
    "plt.plot(x, y, label='Mean Validation Accuracy', color='orange', marker = '+')\n",
    "plt.title('Model Validation Accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf3484-4b86-4f98-9f72-5609749162b4",
   "metadata": {},
   "source": [
    "**Comment:** Fine Tuned Model name is retrieved here! This can be called in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3085e20-d650-4997-a9d6-8ee584736e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(fine_tune_job_id):\n",
    "    fine_tune_response = client.fine_tuning.jobs.retrieve(fine_tune_job_id)\n",
    "    fine_tuned_model_name = fine_tune_response.fine_tuned_model\n",
    "    print(fine_tuned_model_name)\n",
    "\n",
    "    return fine_tuned_model_name\n",
    "\n",
    "ft_model_name = get_model_name(ftj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431edf4f-7627-4434-9af0-35ef0d1099d2",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "---\n",
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d72cd-1312-4883-ad78-c61799c219ec",
   "metadata": {},
   "source": [
    "**Comment:** testing data needs to be loaded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c87db3-7d08-4bc1-b4fe-aafbb4ca93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testing_data(testing_data_path):\n",
    "    \"\"\"\n",
    "    Loads testing data from a JSONL (JSON Lines) file, where each line contains a separate JSON object.\n",
    "\n",
    "    Parameters:\n",
    "    testing_data_path (str): The file path to the testing dataset.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, where each dictionary represents a testing record.\n",
    "\n",
    "    Example:\n",
    "    >>> testing_data = load_testing_data(\"data/testing_dataset.jsonl\")\n",
    "    Loaded 1000 testing records.\n",
    "    \"\"\"\n",
    "    testing_data = []\n",
    "    with open(testing_data_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            testing_data.append(json.loads(line.strip()))\n",
    "    \n",
    "    print(f\"Loaded {len(testing_data)} testing records.\")\n",
    "\n",
    "    return testing_data\n",
    "\n",
    "testing_data = load_testing_data(combined_testing_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f7e14-97bd-45c8-b59f-ef70cf3c71c4",
   "metadata": {},
   "source": [
    "**Comment:** If you would like to see how the trained model is performing, testing data can be used below. Additionally, you can input a base model for comparison. Neither of the model names in the function below need to be a fine tuned model. This is useful if you would like to compare results between 2 base models in the event you would like to fine tune a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c68047-1ef5-461a-8407-e55a260fcc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(testing_data, model_name, base_model_name=None, temperature=0.2):\n",
    "   \"\"\"\n",
    "    Evaluates a fine-tuned language model on a given testing dataset by generating translations\n",
    "    and compares it with a base model's predictions.\n",
    "\n",
    "    Parameters:\n",
    "    testing_data (list[dict]): A list of dictionaries, where each dictionary contains:\n",
    "        - 'english_text' (str): The input English sentence.\n",
    "        - 'spanish_text' (str): The expected Spanish translation.\n",
    "    model_name (str): The name of the fine-tuned model to use for inference.\n",
    "    base_model_name (str, optional): The name of the base model for comparison (default: None).\n",
    "    temperature (float, optional): The temperature setting for model responses, controlling randomness (default: 0.2).\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, where each dictionary contains:\n",
    "        - 'input' (str): The original English text.\n",
    "        - 'expected_output' (str): The expected Spanish translation.\n",
    "        - 'predicted_output' (str): The model-generated Spanish translation.\n",
    "        - 'base_predicted_output' (str, optional): The base model's generated translation.\n",
    "\n",
    "    Example:\n",
    "    >>> predictions = test_model(testing_data=testing_data, model_name=\"ft-gpt-4o\", base_model_name=\"gpt-4o-mini\")\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for test_instance in testing_data:\n",
    "        english_text = test_instance['english_text']\n",
    "        expected_output = test_instance['spanish_text']\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": translation_prompt},\n",
    "                {\"role\": \"user\", \"content\": english_text}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        prediction = response.choices[0].message.content\n",
    "\n",
    "        base_response = client.chat.completions.create(\n",
    "            model=base_model_name,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": translation_prompt},\n",
    "                {\"role\": \"user\", \"content\": english_text}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        base_prediction = base_response.choices[0].message.content\n",
    "        \n",
    "        predictions.append({\n",
    "            'input': english_text,\n",
    "            'expected_output': expected_output,\n",
    "            'predicted_output': prediction,\n",
    "            'base_predicted_output': base_prediction\n",
    "        })\n",
    "\n",
    "    return predictions\n",
    "\n",
    "predictions = test_model(testing_data=testing_data, model_name=ft_model_name, base_model_name='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea870b-21ef-495e-b828-3b221e333c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_scores = [\n",
    "    meteor_score(\n",
    "        [word_tokenize(pred['expected_output'])],  # Tokenized reference\n",
    "        word_tokenize(pred['predicted_output'])   # Tokenized hypothesis\n",
    "    )\n",
    "    for pred in predictions\n",
    "]\n",
    "\n",
    "base_meteor_scores = [\n",
    "     meteor_score(\n",
    "        [word_tokenize(pred['expected_output'])],  # Tokenized reference\n",
    "        word_tokenize(pred['base_predicted_output'])   # Tokenized hypothesis\n",
    "    )\n",
    "    for pred in predictions\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc128a7-f6a3-46ce-b58d-f966e0adc99a",
   "metadata": {},
   "source": [
    "**Comment:** Additional (Unsupervised) Scoring is being worked on. Will be implemented as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb87df6-d0cb-491c-b144-4e69e3d5f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(meteor_scores, label='Fine Tuned Model')\n",
    "plt.plot(base_meteor_scores, label='Base Model')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.2)\n",
    "plt.title('Model Meteor Scoring')\n",
    "plt.show()\n",
    "\n",
    "# to get a better sense of what is happening, both of the scores are sorted independently to show each models trends in scoring.\n",
    "sorted_meteor_scores = sorted(meteor_scores, reverse=True)\n",
    "sorted_base_meteor_scores = sorted(base_meteor_scores, reverse=True)\n",
    "\n",
    "plt.plot(sorted_meteor_scores, label='Fine Tuned Model')\n",
    "plt.plot(sorted_base_meteor_scores, label='Base Model')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.2)\n",
    "plt.title('Model Meteor Scoring (Sorted)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aeb71c-99be-4279-913e-ab89d6ba0f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
